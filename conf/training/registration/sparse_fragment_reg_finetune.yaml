# @package training
defaults:
  - /training/default
  - override optim/optimizer: SGD

epochs: 200
num_workers: 6
batch_size: 4

optim:
    base_lr: 1e-3
    weight_decay: 1e-4
    optimizer:
      momentum: 0.8
      weight_decay: ${training.optim.weight_decay}
    lr_scheduler:
      class: ExponentialLR
      params:
        gamma: 0.999
    bn_scheduler: {}

wandb:
    project: registration

